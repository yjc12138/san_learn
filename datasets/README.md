# Prepare Datasets

A dataset can be used by accessing [DatasetCatalog](https://detectron2.readthedocs.io/modules/data.html#detectron2.data.DatasetCatalog)
for its data, or [MetadataCatalog](https://detectron2.readthedocs.io/modules/data.html#detectron2.data.MetadataCatalog) for its metadata (class names, etc).
This document explains how to setup the builtin datasets so they can be used by the above APIs.
[Use Custom Datasets](https://detectron2.readthedocs.io/tutorials/datasets.html) gives a deeper dive on how to use `DatasetCatalog` and `MetadataCatalog`,
and how to add new datasets to them.

The datasets are assumed to exist in a directory specified by the environment variable
`DETECTRON2_DATASETS`.
Under this directory, detectron2 will look for datasets in the structure described below, if needed.
```
$DETECTRON2_DATASETS/
  coco/                   # COCO-Stuff
  ADEChallengeData2016/   # ADE20K-150
  ADE20K_2021_17_01/      # ADE20K-847
  VOCdevkit/ 
    VOC2010/              # PASCAL Context
    VOC2012/              # PASCAL VOC
```

You can set the location for builtin datasets by `export DETECTRON2_DATASETS=/path/to/datasets`.
If left unset, the default is `./datasets` relative to your current working directory.

## Prepare data for [COCO-Stuff](https://github.com/nightrome/cocostuff):

### Expected data structure

```
coco/
  stuffthingmaps/
    train2017/
    val2017/
  train2017/
  val2017/
  # below are generated by prepare_coco_stuff.py
  stuffthingmaps_detectron2/
    train2017/
    val2017/ 
```
Download the COCO (2017) images from https://cocodataset.org/

```bash
wget http://images.cocodataset.org/zips/train2017.zip
wget http://images.cocodataset.org/zips/val2017.zip
```

Download the COCO-Stuff annotation from https://github.com/nightrome/cocostuff.
```bash
wget http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/stuffthingmaps_trainval2017.zip
```
Unzip `train2017.zip`, `val2017.zip`, and `stuffthingmaps_trainval2017.zip`. Then put them to the correct location listed above. and generate the labels for training and testing.

```
python datasets/prepare_coco_stuff.py
```
or

Format the data to d2 style and split it into Seen (Base) subset and Unseen (Novel) subset.
       
```bash
python datasets/prepare_coco_stuff_164k_sem_seg.py
```   


## Prepare data for [ADE20K-150](http://sceneparsing.csail.mit.edu):

### Expected data structure 
```
ADEChallengeData2016/
  annotations/
    training/
    validation/
  images/
    training/
    validation/
  # below are generated by prepare_ade20k_150.py
  stuffthingmaps_detectron2/
    training/
    validation/
```
Download the data of ADE20K-150 from http://sceneparsing.csail.mit.edu.
```
wget http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip
```
Unzip `ADEChallengeData2016.zip` and generate the labels for testing.
```
python datasets/prepare_ade20k_150.py
```
## Prepare data for [ADE20k-847](https://groups.csail.mit.edu/vision/datasets/ADE20K/):

### Expected data structure 
```
ADE20K_2021_17_01/
  images/
    ADE/
      validation/
  index_ade20k.mat
  index_ade20k.pkl
  # below are generated by prepare_ade20k_847.py
  annotations_detectron2/
    validation/
```
Download the data of ADE20k-Full from https://groups.csail.mit.edu/vision/datasets/ADE20K/request_data/
Unzip the dataset and generate the labels for testing.
```
python datasets/prepare_ade20k_847.py
```

## Prepare data for [PASCAL VOC 2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#devkit):


### Expected data structure 
```
VOCdevkit/
  VOC2012/
    Annotations/
    ImageSets/
    JPEGImages/
    SegmentationClass/
    SegmentationClassAug/ 
    SegmentationObject/
    # generated by prepare_voc.py or prepare_voc_sem_seg.py
    annotations_detectron2
    # generated by prepare_voc.py
    annotations_detectron2_bg
```
Download the data of PASCAL VOC from http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#devkit.

We use SBD augmentated training data as SegmentationClassAug following [Deeplab](https://github.com/kazuto1011/deeplab-pytorch/blob/master/data/datasets/voc12/README.md).
```
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
wget https://www.dropbox.com/s/oeu149j8qtbs1x0/SegmentationClassAug.zip
```
Unzip `VOCtrainval_11-May-2012.tar` and `SegmentationClassAug.zip`. Then put them to the correct location listed above and generate the labels for testing.
```
python datasets/prepare_voc.py
```

or

Download data from the offical dataset website and extract it like below.
```bash
VOCdevkit/
  VOC2012/
    #download http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar and extract it here.
    JPEGImages/
    # copy `ImageSets/Segmentation/val.txt` here.
    val.txt
    #Download auged annotations from http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz and convert the original mat file to png format. Or download it from https://www.dropbox.com/s/oeu149j8qtbs1x0/SegmentationClassAug.zip?dl=0 (Provided in https://github.com/DrSleep/tensorflow-deeplab-resnet#evaluation).
    SegmentationClassAug/
    #https://gist.githubusercontent.com/sun11/2dbda6b31acc7c6292d14a872d0c90b7/raw/5f5a5270089239ef2f6b65b1cc55208355b5acca/trainaug.txt
    train.txt
    # below are generated by prepare_voc_sem_seg.py
    annotations_detectron2
    image_detectron2
```
Format the data to d2 style and split it into Seen (Base) subset and Unseen (Novel) subset.
```bash
python datasets/prepare_voc_sem_seg.py
```


## Prepare data for [PASCAL Context](https://www.cs.stanford.edu/~roozbeh/pascal-context/):


### Expected data structure 
```
VOCdevkit/
  VOC2010/
    Annotations/
    ImageSets/
    JPEGImages/
    SegmentationClass/
    SegmentationObject/
    trainval/
    labels.txt
    # copy from this dir
    pascalcontext_val.txt
    # download
    trainval_merged.json
    # below are generated by prepare_pascal_context_59.py and prepare_pascal_context_459.py
    annotations_detectron2/
      pc459_val
      pc59_val
```
Download the data of PASCAL VOC 2010 from https://www.cs.stanford.edu/~roozbeh/pascal-context/. 

```
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2010/VOCtrainval_03-May-2010.tar
```
Download the annotation for [59](https://codalabuser.blob.core.windows.net/public/trainval_merged.json) and [459](https://roozbehm.info/pascal-context/trainval.tar.gz) classes.
```
wget https://codalabuser.blob.core.windows.net/public/trainval_merged.json
wget https://roozbehm.info/pascal-context/trainval.tar.gz
```
Unzip `VOCtrainval_03-May-2010.tar` and `trainval.tar.gz`. Then put them to the correct location listed above and generate the labels for testing.
```
python datasets/prepare_pascal_context_59.py
python datasets/prepare_pascal_context_459.py
```